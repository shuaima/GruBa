\section{User Clustering}
\label{sec:uc}

%clear points
%road map
%detail of each step, with motivation
%proof if applicable
%self-explain

\par Providing the \textit{Feature Data}, the User Clustering module takes the charge of grouping each user concerned into a proper cluster,
as illustrated in  Algorithm \ref{alg:uc}.
%
The idea is to enumerate a number of clustering trials (line \ref{enu}) and select the optimal solution with the best Silhouette coefficient value ($v$ in line \ref{opti}).
In principle, each trial (referred by $t$ in line \ref{enu}) first performs a clustering task (line \ref{task} to be detailed in section \ref{sec:cluster}), resulting a cluster (by $l(u)$) for each user $u$ (line \ref{l});
then, each user obtains a Silhouette coefficient value $v(u)$ stemmed from the in/out-cluster distances (lines \ref{v-b}--\ref{v-e}; shall be illustrated in section \ref{sec:compu});
finally, the averaged Silhouette coefficient value of all users serves as the Silhouette coefficient value of the current trial, written as $v(t)$ (line \ref{avg}), by which the said selection process is conducted (line \ref{opti}).


\begin{algorithm}[t]
\begin{small}
\caption{User Clustering in \sys{}}
\label{alg:uc}
\begin{algorithmic}[1]
\State Input: \textit{Feature Data} $F$  of a set of users, the minimum/maximum number of clusters $N_i$ and $N_a$
\State Output: Optimal user clustering result $R$
\vspace{1ex}
\ForAll {$t \in [N_i, N_a]$} \label{enu}
	%\State group users into $t$ clusters by $F$;
    \State perform K-Gru over F to get t clusters;\label{task}
	\State clustering result $R'(t)\ =\ \{(u,\ l(u))\}$ with cluster info $l(u)$ for each user $u$; \label{l}
	\ForAll {$u \in \{u\}$}
		\State in-cluster distance $d_i(u)$ \label{v-b};
		\State out-cluster distance $d_o(u)$;
		\State Silhouette coefficient value $v(u)\ :=\ \frac{(d_o - d_i)}{max(d_o,\ d_i)}$; \label{v-e}
	\EndFor
	\State $v(t)\ :=\ Avg\{v(u)\}$; \label{avg}
\EndFor
\If {$v(a)\ =\ \ Max\{v(t)\}$} \label{opti}
	\State $R$ := $R'(a)$;
\EndIf
\State \bfseries{return} $R$.
\end{algorithmic}
\end{small}
\end{algorithm}


Next, we shall now first detail how \sys{} performs the clustering task and subsequently illustrate the computation for the metric of Silhouette coefficient value.

\subsection{K-Gru: Clustering in \sys{}}
\label{sec:cluster}

In \sys{}, the clustering rests on an optimized K-Prototype \cite{IEEEexample:huang1997clustering} algorithm, named K-Gru in this work.
Similar as K-Prototype, K-Gru randomly selects the cluster kernels among samples and employs the minimum distance between them to determine an initial result, upon which the clustering tasks are iterated until the results are stable.

Unlike K-Prototype that supports vector samples in which each dimension is of numerical/categorical, K-Gru could also handle the case where a dimension is one normalized vector.
%
Recall the sample data for User Clustering, i.e., \textit{Feature Data} in form of vectors (see formula \ref{eq:fu}), of which the data type regarding each dimension is shown as Table \ref{tbl:data-type}.

\begin{table}[tb!]
\centering
\begin{small}
%\caption{Types of Dimensional Data in \textit{Feature Data} Vector}
\caption{Dimension Types in \textit{Feature Data} Vector}
\vspace{0.1cm}
\label{tbl:data-type}
\begin{tabular}{ll}
\toprule
\multicolumn{1}{l}{\textbf{Types}} & \multicolumn{1}{l}{\textbf{Data Dimensions}}	\\	\midrule \midrule
numerical data				& $\#R_u$, $\#E_u$, $R_{ee,u}$, $\#B_u$, $R_{oc,u}$, $\#W_{r,u}$,$\#W_{t,u}$               \\	\midrule 				
categorical data			& $G_u, P_u, U_{t,u}$				\\	\midrule
normalized vectors			& $P_{rt,u}$, $P_{rg,u}$, $P_{tt,u}$, $P_{tg,u}$, $P_u$			\\ \bottomrule
\end{tabular}
\end{small}
\end{table}

As aforementioned, the clustering of K-Gru rests on the distance between vector samples, where the dimensions are combined with numbers (normalized to 0-1 range), categories and normalized vectors. For simplicity, we shall first illustrate the distance calculation of the simple vectors with a mono data type on each dimension, and then demonstrate that of complex vectors.

For numerical vectors $Y'\ = (y'_0, y'_1, \ldots)$ and $Z'\ = (z'_0, z'_1, \ldots)$, the \od{} \cite{IEEEexample:books/mk/HanKP2011} between $Y'$ and $Z'$ is given by :
%
\begin{equation}
\label{eq:od}
D_n(Y', Z') = \sum_{\substack{e}} (y'_e - z'_e)^2.
\end{equation}

For categorical vectors $Y''\ = (y''_0, y''_1, \ldots)$ and  $Z''\ = (z''_0, z''_1, \ldots)$, the \hd{} \cite{IEEEexample:huang1997clustering} of $Y''$ and $Z''$ is:
%
\begin{equation}
\label{eq:hd}
D_h(Y'', Z'') = \sum_{\substack{e}} H_e,
\end{equation}
where $H_e$ refers to the \hd{} over each dimension, with $H_e = 0$ if $y''_e$ and $z''_e$ share the identical value, and $H_e = 1$, otherwise.

Regarding two vectors where each dimension is a normalized vector per se, Cosine Similarity is leveraged to compute the distance.
Then, the distance between such two vectors $Y^{\ast}\ = (Y^{\ast}_0, Y^{\ast}_1, ...)$ and $Z^{\ast}\ = (Z^{\ast}_0, Z^{\ast}_1, ...)$ is:
%
\begin{equation}
\label{eq:vd}
D_v(Y^{\ast}, Z^{\ast}) = \sum_{\substack{e}} (1- Y^{\ast}_e \cdot Z^{\ast}_e),
\end{equation}
where $\cdot$ refers to the dot product operation between two normalized vectors $Y^{\ast}_e$ and $Z^{\ast}_e$.

Putting these together, the distance regarding the complex vectors $Y\ =\ (Y_0, Y_1, ...)$ and $Z\ =\ (Z_0, Z_1, ...)$ in K-Gru, referred to as \gd{}, is defined as:
%
\begin{equation}
\label{eq:gd}
D_g(Y, Z) = \sum_{\substack{e}} G_e,
\end{equation}
where the distance on each dimension $G_e$ is given by:
%
\begin{equation}
\label{eq:ge}
G_e =
  \begin{cases}
    (Y_e - Z_e)^2       &  \text{if } Y_e/Z_e \text{ is numerical}\\
    H_e\ (1\ or\ 0)       	&  \text{if } Y_e/Z_e \text{ is categorical}\\
    1 - Y_e \cdot Z_e  		&  \text{if } Y_e/Z_e \text{ is of normalized vector}
  \end{cases}
\end{equation}


\subsection{Silhouette Coefficient Metric Computation}
\label{sec:compu}

In system \sys{}, the Silhouette coefficient metric serves as the fundamental criteria for deriving an optimal clustering result.
Providing a clustering result, each user is associated with a cluster.

%For a user $u$ of cluster $l$, we employ the vector $Y$ to denote the \textit{Feature Data} as in formula \ref{eq:fu}.

\begin{definition}
\label{def:di}
The in-cluster distance $d_i(u)$ is the average distance to all the other users in the same cluster:
\begin{equation}
	d_i(u) = Avg\{D_g(F_u, F_{u''})\ |\ u'' \in l \wedge  u \neq u''\}.
\end{equation}
\end{definition}

\begin{definition}
\label{def:do}
The out-cluster distance $d_o(u)$ is measured as the minimum of the distances $\{d^{\ast}\}$ between $u$ and other clusters:
\begin{equation}
	d_o(u) = Min\{d^{\ast}(u, l')\ |\  l' \neq l \},
\end{equation}
where $d^{\ast}$ is given by:
\begin{equation}
	d^{\ast}(u, l') = Avg\{D_g(Y_u, Y_{u'}) \ | \ u' \in l'\}.
\end{equation}
\end{definition}

\begin{definition}
\label{def:coef}
The Silhouette coefficient value $v(u)$ is defined as:
\begin{equation}
\label{eq:coef}
v(u)\ =\ \frac{(d_o - d_i)}{max(d_o,\ d_i)}.	
\end{equation}
\end{definition}

Intuitively, a good clustering solution should have a bigger $d_o$ and a smaller $d_i$, such that samples with obvious differences go to various clusters and vice versa.
When $d_o$ is far more than $d_i$, Silhouette coefficient value approaches to 1.
Hence, the larger Silhouette coefficient value is, the better clustering performs, by which the optimal solution is selected.





