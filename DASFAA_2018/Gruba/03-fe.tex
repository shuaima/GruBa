\begin{table}[h!]
\begin{minipage}[b]{0.6\linewidth}
\centering
\begin{small}
\caption{Illustration of Variables in Basic Feature}
\vspace{0.1cm}
\label{tbl:fe-info}
\begin{tabular}{ll}
\toprule
\multicolumn{1}{l}{\textbf{Variables}} & \multicolumn{1}{l}{\textbf{Illustration}}	\\	\midrule \midrule
$G_u$				& gender of user $u$				\\	\midrule
$P_u$				& province of user $u$				\\	\midrule
\#$R_u$				& number of followers				\\	\midrule
\#$E_u$				& number of followees				\\	\midrule
                    & a ratio defined as the number of             \\	
\raisebox{1.5ex}{$R_{ee,u}$} &followers over that of followees,i.e.,$\frac{\#R_u}{\#E_u}$                  \\  \midrule
$U_{t,u}$		    & user type (as illustrated in Table \ref{tbl:ucate})      \\ \bottomrule		
\end{tabular}
\end{small}
\vspace{-1ex}
\end{minipage}
\begin{minipage}[b]{0.4\linewidth}
\centering
\begin{small}
\caption{Category of User Type}
\vspace{0.1cm}
\label{tbl:ucate}
\begin{tabular}{ll}
\toprule
\multicolumn{1}{l}{\textbf{Types}} & \multicolumn{1}{l}{\textbf{Illustration}}	\\	\midrule \midrule
0                       & $\#E_u \le 50 \ {\bf and}$   \\
                        & $\ \#R_u \le 50$				  \\	\midrule
\multirow{2}{*}{1}      & \multirow{2}{*}{$\frac{\#E_u}{\#R_u} \ \ge 5$}	\\
 						&                       									\\	\midrule
\multirow{2}{*}{2}		& \multirow{2}{*}{$\frac{\#R_u}{\#E_u} \ \ge 5$}  \\
						&															\\	\midrule
3                   	& other cases 												\\ \bottomrule
\end{tabular}
\end{small}
\vspace{-1ex}
\end{minipage}
\end{table}

\section{Feature Extraction}
\label{sec:fe}

\par With the underlying Sina Microblog Data, the Feature Extraction module is responsible for mining the user characteristics, and produces three classes of features for each user: \textit{Basic Feature}, \textit{Behavior Feature} and \textit{Interest Feature}, referred to as  \textit{Feature Data} in \sys{}.
%, i.e., \textit{Feature Data} = \{\textit{Basic Feature}, \textit{Behavior Feature}, \textit{Interest Feature}\}.

\subsection{Basic Feature}

The \textit{Basic Feature} employs a vector $I$ to depict the basic characteristics of a user $u$.
\begin{equation}
\label{eq:info}
	I_u = (G_u, P_u, \#R_u, \#E_u, R_{ee,u}, U_{t,u}),
\end{equation}
in which the variable details are illustrated in Table \ref{tbl:fe-info}.
%Specifically, we use $\#(B_s | R(B)==1)$ and $\#(B_s | R(B)==0)$ to represent the number of \retd{} microblogs and microblogs that are originally created by the user.

\subsection{Behavior Feature}

Unlike \textit{Basic Feature}, the \textit{Behavior Feature} of a user $u$ is certain statistics regarding the \retg{} behavior of $u$, shown below:

\sstab(a) the number of owned microblogs $\#B_u$,

\sstab(b) the ratio $R_{oc,u}$ that is the number of \retd{} microblogs over that of originally \twd{}, i.e., $\frac{\#(B_u | flag ==1)}{\#(B_u | flag ==0)}$,

\sstab(c) the average number of \retd{} and \twd{} microblogs per week: $\#W_{r,u}$ and $\#W_{t,u}$,

\sstab(d) the normalized vectors regarding the time distribution of a user's \retg{}/\twg{} behavior: $P_{rt,u}\ = (p'_{r0},\ p'_{r1},\ ...,\ p'_{r11})$, $P_{tt,u}\ = (p'_{t0},\ p'_{t1},\ ...,\ p'_{t11})$, where $p'_{r0}$/$p'_{t0}$ is the probability that the \retg{}/\twg{} activity happens from 0am to 2am, $p'_{r1}$/$p'_{t1}$ is the probability that the \retg{}/\twg{} activity happens from 2am to 4am, and so on, and

\sstab(e) the normalized vectors with respect to the gap distribution of a user's \retg{}/\twg{} behavior: $P_{rg,u}\ = (p''_{r0},\ p''_{r1},\ ...,\ p''_{r5})$, $P_{tg,u}\ = (p''_{t0},\ p''_{t1},\ ...,\ p''_{t5})$, in which $p''_{r0}$/$p''_{t0}$ is the probability that the gap between two \retd{}/\twg{} microblogs is within 1 min. Ditto for $p''_{r1}$/$p''_{t1}$ (1 min to 1 hour), $p''_{r2}$/$p''_{t2}$ (1 to 12 hours), $p''_{r3}$/$p''_{t3}$ (12 to 24 hours), $p''_{r4}$/$p''_{t4}$ (24 to 48 hours) and  $p''_{r5}$/$p''_{t5}$ (more than 48 hours).


In summary, the \textit{Behavior Feature} $H_u$ of user $u$ consists of the following:

\begin{equation}
\label{eq:beha}
(\#B_u, R_{oc,u}, \#W_{r,u}, P_{rt,u}, P_{rg,u}, \#W_{t,u}, P_{tt,u}, P_{tg,u}).
\end{equation}


% where $\#B_u$, $R_{oc}$, $\#W_r$, $P_t$ and $P_g$ are illustrated as above.

\subsection{Interest Feature}

Different from the slightly straightforward \textit{Basic Feature} and \textit{Behavior Feature}, \textit{Interest Feature} involves a process of labeling users with their interested topics based on their tweeted and retweeted microblogs.
In short, with a given lexicon (made by some professionals) consisting of several \textit{topics}, the interest feature of a user $u$ is a normalized vector, in which each entry refers to the degree that $u$ is interested in the corresponding \textit{topic}.

\begin{definition}
\label{def:lexi}
A lexicon $L$ consists of a set of topics $t$ such that each topic is associated with a set of cell words $c$, in which each cell word depicts an aspect of the topic.
\end{definition}

\begin{definition}
\label{def:bw}
%used \label{def:blog} previously
For a user $u$, each microblog $b \in B_u$ is represented by a set of words $w$.
\end{definition}

\begin{definition}
\label{def:inte}
The interest feature $P_u$ of a user $u$ is a normalized vector
\begin{equation}
\label{eq:inte}
(p_0, p_1, \ldots, p_{x-1}),
\end{equation}
in which user $u$ matches $x$ \textit{topics} in lexicon $L$, and $p_i$ refers to the degree that $u$ is interested in topic $i$.
\end{definition}

The similarity $p_i$¡¡$(i\in[0, x-1])$ will be detailed in each scenario (explicit/implicit interest analysis, towards words/topics, etc).


In  \sys{}, a word, either in the form of $c$ or $w$, acts as the minimum unit for analysis.
Hence, the similarity $sim(w, c)$ of a word pair ($w$, $c$) could be generalized to the similarity of a microblog against one topic $sim(b, t)$, and finally to a user $u$ versus each topic $t$ in lexicon $sim(u, t)$; topics with similarity satisfying certain thresholds are allocated to user $u$ and constitute the interests of $u$.

System \sys{} employs a well established lexicon to discover the explicit interests of users.
%In case when no proper explicit interests are found, TF-IDF (Term-Frequency and Inverse Document-Frequency) and Twitter-LDA %\cite{IEEEexample:zhao2011comparing} are leveraged to explore the implicit interests, during which word2vector %\cite{IEEEexample:mikolov2013distributed} is adopted to measure the similarity between two words.
When no proper explicit interests are found, two algorithms are leveraged to identify implicit interests: (1) TF-IDF (Term-Frequency and Inverse Document-Frequency) \cite{IEEEexample:books/cu/LeskovecRU14}, and (2) Twitter-LDA \cite{IEEEexample:zhao2011comparing} (a method to discover topics from Twitter, by applying the standard Latent Dirichlet Allocation (LDA) model \cite{IEEEexample:conf/jmlr/Blei03}), both of which adopt word2vector \cite{IEEEexample:mikolov2013distributed} to measure word similarity.

We now explain the detailed process for mining the interest features of a user.

%For instance, the following steps depict the ``mining'' process of the interest feature $P_f$ for a user $u$.

\stitle{Step 1:} Each microblog $b$ in $B_u$ of user $u$ is decomposed into a word set $WS$. %, i.e., $b = \{w\}$ where $b \in B_s(u)$.

\stitle{Step 2:} Explicit interests are explored. Specifically, every word $w$ in $WS$ is sent to match each cell word $c$ of lexicon topics.
If $w$ and $c$ are identical, $sim(w, c) = 1$, and $sim(w, c) = 0$, otherwise.

The similarity of $b$ with a lexicon topic $t$ is defined as
\begin{equation}
\label{eq:bt}
sim(b, t) = \sum_{\substack{i, j}} sim(w_i, c_j).
\end{equation}

%where $sim(w_i, c_j)$ refers to the similarity of a word pair.

If $sim(b, t)$ satisfies a certain threshold (3 by default), topic $t$ is labeled to microblog $b$;
the user $u$ is then discovered having an explicit interest (topic) $t$.
Thus, by looking into the similarity of $b$ against all topics in lexicon $L$, the explicit interests of $u$ is derived, in the form of interest feature (Definition \ref{def:inte}).


\stitle{Step 3:}. If the $sim(b, t)$ in {\bf Step 2} cannot meet the threshold, i.e., explicit interest discovery over user $u$ fails, the implicit interests of user $u$ are further mined, by running the following steps 3.1$\&$3.2 in parallel.

\etitle{Step 3.1:} A metric \textit{TF-IDF weight} $W_f$ is computed by employing TF-IDF to calculate the weight distribution of words in microblog $b$:
\begin{equation}
\label{eq:tf}
W_f = \{(w_i, p_i)\},
\end{equation}
where $w_i$ refers to a single word, of which the weight is $p_i$, with $\sum_{\substack{i}} p_i = 1$.

To compute such weight $p_i$ for word $w_i$, a metric $p''_i$ is first calculated as:
\begin{equation}
\label{eq:tf-w}
p''_i = \frac{|b_i|}{|b|} * \log(\frac{|D|}{|D_i|+1}),
\end{equation}
in which we use the operator $|\ |$ to measure the cardinality, such that $|b_i|$ is the occurrences of word $w_i$ in microblog $b$, and $|b|$ the total occurrences of all words in $b$.
$|D|$ is the total number of microblogs in the dataset and $|D_i|$ is the number of microblogs where $|b_i|$ appears.
Hence, each word $w_i$ shall get an initial weight of $p''_i$, upon which the normalization is performed and $p_i$ is obtained, resulting the \textit{TF-IDF weight}.


TF-IDF based similarity is then calculated.
%For example, the similarity (in the form of a value) of $W_f$ over a single topic $t$ in lexicon, written as $sim(W_f, t)$, is defined %as:
%\begin{equation}
%\label{eq:sim-tf1}
%sim(W_f, t) = \sum_{\substack{i}} p_i*sim(w_i, t),
%\end{equation}
%where $W_f = \{(w_i, p_i)\}$, $t = \{c_j\}$, and $sim(w_i, t)$ is the averaged word similarity $sim(w_i, c_j)$ returned by word2vector %\cite{IEEEexample:mikolov2013distributed}.
For example, the similarity of $W_f$ over a single topic $t$ in lexicon, written as $sim(W_f, t)$, is defined as formula \ref{eq:sim-tf1}.
Here $W_f = \{(w_i, p_i)\}$, $t = \{c_j\}$, $VEC_{W_f}$ is defined as formula \ref{eq:sum-tf1}, $VEC_t$ is defined as formula \ref{eq:sum-lexion},
where $N_t$ is the number of words in topic $t$ and $vec[w]$ is the word vector of word $w$ returned by word2vector \cite{IEEEexample:mikolov2013distributed}.
\begin{equation}
\label{eq:sim-tf1}
sim(W_f, t) = VEC_{W_f} \cdot VEC_t, where
\end{equation}
\begin{equation}
\label{eq:sum-tf1}
VEC_{W_f} = \sum_{\substack{i}} p_i*vec[w_i], and
\end{equation}
\begin{equation}
\label{eq:sum-lexion}
VEC_t = \sum_{\substack{j}} \frac{1}{N_t}*vec[c_j].
\end{equation}
%where $W_f = \{(w_i, p_i)\}$, $t = \{c_j\}$, and $sim(w_i, t)$ is the averaged word similarity $sim(w_i, c_j)$ returned by word2vector %\cite{IEEEexample:mikolov2013distributed}.

\etitle{Step 3.2:} Similarly, another metric \textit{Twitter-LDA weight} $W_w$ is obtained by using Twitter-LDA  %\ref{IEEEexample:zhao2011comparing}
to result the word weight distribution of microblog $b$.
Unlike TF-IDF, Twitter-LDA first trains the overall microblogs, allocating each microblog with a \textit{tag}.
%
The structure of \textit{tag} is as follows:
\begin{equation}
\label{eq:tw-tag}
W_t = \{(w'_i, p'_i)\},
\end{equation}
where $w'_i$ refers to a word in \textit{tag} $W_t$, and $p'_i$ is the probability that $w'_i$ appears in microblogs with the said \textit{tag}, with $\sum_{\substack{i}} p'_i = 1$ ($|W_t|\ =\ 30$ in this work by default).
%
Subsequently, $W_t$ are leveraged to conclude $W_w$, i.e., $W_w\ =\ W_t$ , which shares the format with that of $W_f$.

\stitle{Step 4:} Hence, the similarity of a microblog $b$ against a lexicon topic $t$ is :
\begin{equation}
\label{eq:simbt}
sim(b, t) = \alpha * sim(W_f, t) + (1 - \alpha) * sim(W_t, t),
\end{equation}
where the $\alpha$ is a parameter by which \sys{} could set flexible priorities between TF-IDF and Twitter-LDA.

\stitle{Step 5:} Repeat Steps 1 to 4 for the microblog $b$ over every topic in lexicon, i.e., $\forall t_k \in L$ results one similarity value of $sim(b, t_k)$.
Such computation further extends to all the microblogs owned by user $u$, such that:
$\forall b_m \in B_s(u)$, $\forall t_k \in L$, there exists a similarity of $sim(b_m, t_k)$.
Hence, the overall similarity of user $u$ over lexicon topics $\{t\}$ (i.e., $L$), written as $S(u,\ L)$, could be denoted by a vector:
\begin{equation}
\label{eq:simul}
S(u, L) = (s_0, s_1, \ldots,\ s_{n-1}),
\end{equation}
where $n$ refers to the cardinality of $L$ (i.e., number of topics in $L$) and $s_k$ is the overall similarity of user $u$ over topic $t_k$, which is given by:
\begin{equation}
\label{eq:simul-2}
s_k = \sum_{\substack{m}} sim(b_m, t_k).
\end{equation}

Among the $n$ dimensions of $S(u,\ L)$, those with top $x$ (3 in \sys{}) similarity values are selected to label the implicit interests of user $u$, which results an $x$ dimensional vector $P_u$ as described in Definition \ref{def:inte}.
Similarly, interest features of all users are returned.

As a result, the \textit{Feature Data} $F_u$ for a user $u$ is
\begin{equation}
\label{eq:fu}
	F_u = (I_u, H_u, P_u),
\end{equation}
where $I_u$, $H_u$ and $P_u$ are the \textit{Basic Feature}, \textit{Behavior Feature} and \textit{Interest Feature} of $u$, respectively.

%And it could be written as a vector
%\begin{equation}
%\label{eq:fu-flat}
%\begin{aligned}
%	F(u) = (G_u, P_u, \#R_s, \#E_s, R_{ee}, U_t, \#B_u, R_{oc},\\
%    \#W_r, P_{rt}, P_{rg}, \#W_t, P_{tt}, P_{tg}, P_f),
%(\#R_s, \#E_s, R_{ee}, \#B_s, R_{oc}, U_t, \#W_r, P_t, P_g, P_f)
%\end{aligned}
%\end{equation}
%where each entry refers to a data item of \sys{}.

